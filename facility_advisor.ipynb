{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XLSX -> CSV -> DOCX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pandas\n",
    "\n",
    "df = pandas.read_excel('data/unstructured/yapayZekaVeriMedicalCrm.xlsx')\n",
    "\n",
    "\n",
    "df = pandas.read_csv('data/unstructured/yapayZekaVeriMedicalCrm.csv')\n",
    "\n",
    "# remove except istanbul, ankara, izmir, bursa, antalya\n",
    "df = df[df['SEHIR'].isin(['İstanbul', 'Ankara', 'İzmir', 'Bursa', 'Antalya'])]\n",
    "df.to_csv('data/unstructured/yapayZekaVeriMedicalCrm.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove duplicates\n",
    "df['KURUM'].drop_duplicates().to_csv('data/unstructured/yapayZekaVeriMedicalCrmKurum.csv', index=False)\n",
    "\n",
    "# remove except these KURUM's \n",
    "#Başkent University Ankara Hospital\n",
    "#Dünyagöz Ankara Hospital\n",
    "#Çankaya Hospital\n",
    "#Koru Hospital\n",
    "\n",
    "#Akdeniz Şifa Hospital\n",
    "#Medicalpark Antalya Hospital Complex\n",
    "#Memorial Antalya Hospital\n",
    "#Talya Medical Center\n",
    "\n",
    "#Acıbadem University  Acıbadem Atakent Hospital\n",
    "#Avrasya Hospital\n",
    "#Medicabil Hospital\n",
    "#Aritmi Osmangazi Hospital\n",
    "\n",
    "#Başkent University İstanbul Health Research & Application Center\n",
    "#Acıbadem Bakırköy Hospital\n",
    "#Apeks Medical Center\n",
    "#Ataşehir Florence Nightıngale Hospital\n",
    "\n",
    "#Park Medical Center\n",
    "#Ege City Hospital\n",
    "#Kent Hospital\n",
    "#Egepol Cerrahi Hospital\n",
    "\n",
    "df = df[df['KURUM'].isin([  'Başkent University Ankara Hospital',\n",
    "                            'Dünyagöz Ankara Hospital',\n",
    "                            'Çankaya Hospital', 'Koru Hospital',\n",
    "                            'Akdeniz Şifa Hospital', \n",
    "                            'Medicalpark Antalya Hospital Complex', \n",
    "                            'Memorial Antalya Hospital', 'Talya Medical Center', \n",
    "                            'Acıbadem University  Acıbadem Atakent Hospital',\n",
    "                            'Avrasya Hospital', 'Medicabil Hospital', \n",
    "                            'Aritmi Osmangazi Hospital', \n",
    "                            'Başkent University İstanbul Health Research & Application Center', \n",
    "                            'Acıbadem Bakırköy Hospital', \n",
    "                            'Apeks Medical Center', \n",
    "                            'Ataşehir Florence Nightıngale Hospital', \n",
    "                            'Park Medical Center', \n",
    "                            'Ege City Hospital', \n",
    "                            'Kent Hospital', \n",
    "                            'Egepol Cerrahi Hospital'])]\n",
    "df = df.dropna(subset=['KURUM', 'SEHIR', 'TEDAVI', 'UZMANLIK'])\n",
    "df.to_csv('data/structured/yapayZekaVeriMedicalCrm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the csv file into text following structure => ['KURUM'] is in ['SEHIR'] and gives a treatment for ['TEDAVI'] about ['UZMANLIK']\n",
    "sentences = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sentence = f\"[{row['KURUM']}](facility_name) is in [{row['SEHIR']}](facility_place) and gives a treatment for [{row['TEDAVI']}](operation_name) about [{row['UZMANLIK']}](operation_type)\"   \n",
    "    sentences.append(sentence)\n",
    "text_data = '\\n'.join(sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "doc = Document()\n",
    "doc.add_paragraph(text_data)\n",
    "doc.save('data/structured/yapayZekaVeriMedicalCrm.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENTITIY RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('env/.env', override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAHA GELMİŞ BİR ENTİTY RECOGNITION İÇİN TEK YAPMAMIZ GEREKEN PROMT'LARDAKİ ÖRNEK SAYISINI ARTTIRMAK\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a smart and intelligent Named Entity Recognition (NER) system. I will provide you the definition of the entities you need to extract and the sentence from which you need to extract the entities and the output in given format with examples.\"\n",
    "\n",
    "USER_PROMPT_1 = \"Are you clear about your role?\"\n",
    "\n",
    "ASSISTANT_PROMPT_1 = \"Sure, I'm ready to help you with your NER task. Please provide me with the necessary information to get started.\"\n",
    "\n",
    "PROMPT = (\n",
    "    \"Entity Definition:\\n\"\n",
    "    \"1. KURUM: Brand names that usually contains more than one word.\\n\"\n",
    "    \"2. TEDAVI: Wanted medical operation name.\\n\"\n",
    "    \"3. UZMANLIK: Medical specialty name that contains the TEDAVI.\\n\"\n",
    "    \"4. SEHIR: Name of the cities for operation to occur like İstanbul, Ankara, İzmir, Bursa, Antalya etc.\\n\"\n",
    "\n",
    "    \"Output Format:\\n\"\n",
    "    \"{{'KURUM': [list of entities present], 'UZMANLIK': [list of entities present], 'TEDAVI': [list of entities present],'SEHIR': [list of entities present]}}\\n\"\n",
    "    \"If no entities are presented in any categories keep it None\\n\"\n",
    "    \"\\n\"\n",
    "    \"Examples:\\n\"\n",
    "    \"\\n\"\n",
    "    \"1. Sentence: Antalyada brazilian butt lift yaptırabileceğim bir estetik merkezi var mı?\\n\"\n",
    "    \"Output: {{'KURUM': [], 'TEDAVI': ['Brazilian Butt Lift'], 'UZMANLIK': ['Estetik'],'SEHIR':['Antalya']}}\\n\"\n",
    "    \"\\n\"\n",
    "    \"2. Sentence: Antalya'da diş beyazlatma hizmeti veren bir doktor önerisi istiyorum.\\n\"\n",
    "    \"Output: {{'KURUM': [], 'TEDAVI': [diş beyazlatma], 'UZMANLIK': [diş], 'SEHIR': [antalya]}} \\n\"\n",
    "    \"3. Sentence: Acıbadem University Acıbadem Atakent hastanesi ortopedi bölümünden bilek ameliyatı yaptırmak istiyorum.\\n\"\n",
    "    \"Output: {{'KURUM': [Acıbadem University Acıbadem Atakent Hospital], 'TEDAVI': ['Bilek ameliyatı'], 'UZMANLIK': ['Ortopedi'],'SEHIR':[]}}\\n\"\n",
    "    \"\\n\"\n",
    "    \"4. Sentence: {}\\n\"\n",
    "    \"Output: \"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def openai_chat_completion_response(final_prompt):\n",
    "  response = client.chat.completions.create(\n",
    "              model=\"gpt-3.5-turbo\",\n",
    "              messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": USER_PROMPT_1},\n",
    "                    {\"role\": \"assistant\", \"content\": ASSISTANT_PROMPT_1},\n",
    "                    {\"role\": \"user\", \"content\": final_prompt}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "  return response.choices[0].message.content.strip(\" \\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"{'KURUM': ['Akdeniz Şifa Hospital'], 'TEDAVI': [], 'UZMANLIK': [],'SEHIR':['Antalya']}\", \"{'KURUM': ['Antalya Lara Anadolu Hospital'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Antalya']}\", \"{'KURUM': ['Dr. Gökhan Özerdem Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Antalya']}\", \"{'KURUM': ['Dt. Murat Özbıyık Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Antalya']}\", \"{'KURUM': ['Doç. Dr. Aydın ARSLAN Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Istanbul']}\", \"{'KURUM': ['Doç. Dr. İsmail Küçüker Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Istanbul']}\", \"{'KURUM': ['Dt. Ahmet Sinan Cem ÇEPELOĞLU Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Istanbul']}\", \"{'KURUM': ['Biruni University Health Eğitimi Research & Application Center'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Istanbul']}\", \"{'KURUM': ['Doç. Dr. Esin Yalçınkaya Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Ankara']}\", \"{'KURUM': ['Dr. Dt. Yasemin Özyürek Türkdönmez Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Ankara']}\", \"{'KURUM': ['Fizyocare Phys Therapy & Rehabilitation Center'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Ankara']}\", '{\\'KURUM\\': [\"Başkent University Ankara Hospital\"], \\'TEDAVI\\': [], \\'UZMANLIK\\': [],\\'SEHIR\\':[\\'Ankara\\']}']\n"
     ]
    }
   ],
   "source": [
    "input_sentences = [\n",
    "    \"İstanbul'da göz muayenesi yaptırmak istiyorum.\",\n",
    "    \"Bursa'da botoks enjeksiyonu yaptırmak için bir klinik arıyorum.\",\n",
    "    \"Antalya'da diş beyazlatma hizmeti veren bir doktor önerisi istiyorum.\",\n",
    "    \"Ankara'da saç ekimi yaptırmak istiyorum, uygun bir merkez önerebilir misiniz?\",\n",
    "    \"İzmir'de estetik cerrahi konusunda uzman bir doktor arıyorum.\",\n",
    "    \"İstanbul'da spor yaralanmaları üzerine uzmanlaşmış bir hastane bulabilir misiniz?\",\n",
    "    \"Bursa'da kilo verme ameliyatı yapan bir doktor önerebilir misiniz?\",\n",
    "    \"Antalya'da lazer epilasyon yaptırmak istiyorum, fiyatları hakkında bilgi alabilir miyim?\",\n",
    "    \"Ankara'da çocuk göz hastalıkları konusunda uzman bir doktor arıyorum.\",\n",
    "    \"İzmir'de estetik diş tedavisi yapan bir klinik tavsiye eder misiniz?\",\n",
    "    \"İstanbul'da dermatoloji uzmanı olan bir doktor önerebilir misiniz?\",\n",
    "    \"Bursa'da plastik cerrahi konusunda deneyimli bir doktor araştırıyorum.\",\n",
    "    \"Antalya'da radyoloji testi yaptırabileceğim bir laboratuvar önerisi istiyorum.\",\n",
    "    \"Ankara'da kalp cerrahisi yapan bir hastane arıyorum, tavsiye eder misiniz?\",\n",
    "    \"İzmir'de psikiyatri uzmanı olan bir doktorla görüşmek istiyorum.\"\n",
    "]\n",
    "results = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    formatted_prompt = PROMPT.format(sentence)\n",
    "    result = openai_chat_completion_response(formatted_prompt)\n",
    "    results.append(result)\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İstanbul'da göz muayenesi yaptırmak istiyorum.\n",
      "Bursa'da botoks enjeksiyonu yaptırmak için bir klinik arıyorum.\n",
      "Antalya'da diş beyazlatma hizmeti veren bir doktor önerisi istiyorum.\n",
      "Ankara'da saç ekimi yaptırmak istiyorum, uygun bir merkez önerebilir misiniz?\n",
      "İzmir'de estetik cerrahi konusunda uzman bir doktor arıyorum.\n",
      "İstanbul'da spor yaralanmaları üzerine uzmanlaşmış bir hastane bulabilir misiniz?\n",
      "Bursa'da kilo verme ameliyatı yapan bir doktor önerebilir misiniz?\n",
      "Antalya'da lazer epilasyon yaptırmak istiyorum, fiyatları hakkında bilgi alabilir miyim?\n",
      "Ankara'da çocuk göz hastalıkları konusunda uzman bir doktor arıyorum.\n",
      "İzmir'de estetik diş tedavisi yapan bir klinik tavsiye eder misiniz?\n",
      "İstanbul'da dermatoloji uzmanı olan bir doktor önerebilir misiniz?\n",
      "Bursa'da plastik cerrahi konusunda deneyimli bir doktor araştırıyorum.\n",
      "Antalya'da radyoloji testi yaptırabileceğim bir laboratuvar önerisi istiyorum.\n",
      "Ankara'da kalp cerrahisi yapan bir hastane arıyorum, tavsiye eder misiniz?\n",
      "İzmir'de psikiyatri uzmanı olan bir doktorla görüşmek istiyorum.\n",
      "{'KURUM': ['Akdeniz Şifa Hospital'], 'TEDAVI': [], 'UZMANLIK': [],'SEHIR':['Antalya']}\n",
      "{'KURUM': ['Antalya Lara Anadolu Hospital'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Antalya']}\n",
      "{'KURUM': ['Dr. Gökhan Özerdem Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Antalya']}\n",
      "{'KURUM': ['Dt. Murat Özbıyık Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Antalya']}\n",
      "{'KURUM': ['Doç. Dr. Aydın ARSLAN Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Istanbul']}\n",
      "{'KURUM': ['Doç. Dr. İsmail Küçüker Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Istanbul']}\n",
      "{'KURUM': ['Dt. Ahmet Sinan Cem ÇEPELOĞLU Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Istanbul']}\n",
      "{'KURUM': ['Biruni University Health Eğitimi Research & Application Center'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Istanbul']}\n",
      "{'KURUM': ['Doç. Dr. Esin Yalçınkaya Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Ankara']}\n",
      "{'KURUM': ['Dr. Dt. Yasemin Özyürek Türkdönmez Clinic'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Ankara']}\n",
      "{'KURUM': ['Fizyocare Phys Therapy & Rehabilitation Center'], 'TEDAVI': [], 'UZMANLIK': [], 'SEHIR': ['Ankara']}\n",
      "{'KURUM': [\"Başkent University Ankara Hospital\"], 'TEDAVI': [], 'UZMANLIK': [],'SEHIR':['Ankara']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/structured/yapayZekaVeriMedicalCrm.txt', 'w') as f:\n",
    "    for item in results:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "for sentence in input_sentences:\n",
    "    print(sentence)\n",
    "\n",
    "with open('data/structured/yapayZekaVeriMedicalCrm.txt', 'r') as f:\n",
    "    contents = f.read()\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STORING PDF, DOCX INTO PINECONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL PURPOSE DOCUMENT LOADER FROM PRIVATE DOCUMET\n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    " \n",
    "    data = loader.load()\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHUNKING DATA\n",
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE VECTOR STORE\n",
    "def insert_or_fetch_embeddings(index_name):\n",
    "    import pinecone\n",
    "    from langchain.vectorstores import Pinecone\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "    \n",
    "    if index_name in pinecone.list_indexes():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Done!')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pinecone.create_index(index_name, dimension=1536, metric='cosine')\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Done!')\n",
    "        \n",
    "    return vector_store\n",
    "\n",
    "#DELETE VECTOR STORE\n",
    "def delete_all_pinecone_indexes():\n",
    "    import pinecone\n",
    "    pinecone.init(\n",
    "        api_key = os.environ.get('PINECONE_API_KEY'),\n",
    "        environment = os.environ.get('PINECONE_ENV')\n",
    "    )\n",
    "    indexes = pinecone.list_indexes()\n",
    "    print('Deleting all indexes ...')\n",
    "    for index in indexes:\n",
    "        pinecone.delete_index(index)   \n",
    "    print('Done')\n",
    "\n",
    "#EMBEDDING COST CALCULATOR\n",
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_name_for_model('text-embedding-ada-002')\n",
    "\n",
    "    try:\n",
    "        total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "        print(f'Total Tokens: {total_tokens}')\n",
    "        print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f'Error: {e}. Make sure the \"page_content\" attribute is present in your text pages.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHATBOT OPERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CHATBOT OPERATIONS##\n",
    "\n",
    "# Now you can use vector_store in your ask_and_get_answer function\n",
    "def ask_and_get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k' : 3})\n",
    "    \n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever)\n",
    "    \n",
    "    answer = chain.run(q)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def ask_with_memory(vector_store, q, chat_history=[]):\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k' : 3})\n",
    "    crc = ConversationalRetrievalChain.from_llm(llm,retriever)\n",
    "    result = crc({'question' : q, 'chat_history' : chat_history})\n",
    "    chat_history.append(question, result['answer'])\n",
    "\n",
    "    return result, chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMBEDDING THE JSON INTO PINECONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import JSONLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 12}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "from gensim.models import Word2Vec\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pinecone\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('env/.env', override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "index_name = 'hospitals'\n",
    "delete_all_pinecone_indexes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = json.loads(Path(\"./data/structured/hospitals.json\").read_text()) # load data\n",
    "\n",
    "descriptions = [hospital['Hospital']['Description'] for hospital in data] # get descriptions\n",
    "sentences = [description.split() for description in descriptions] # split descriptions into sentences\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=1536, window=5, min_count=1, workers=4) # train word2vec model (BU SORUN ÇIKARIYOR OLABİLİR BİLMİYORUM)\n",
    "\n",
    "def get_vector(hospital):\n",
    "\n",
    "    description = hospital['Hospital']['Description'].split()\n",
    "    vector = np.mean([model.wv[word] for word in description if word in model.wv.key_to_index], axis=0)\n",
    "\n",
    "    return vector.tolist()\n",
    "\n",
    "ids = [unidecode(hospital['Hospital']['Name']) for hospital in data] # get ids (SADECE ASCII KABUL ETTİĞİ İÇİN UNİDECODE KULLANILDI)\n",
    "vectors = [get_vector(hospital) for hospital in data] # get vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 12}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.create_index(index_name, dimension=1536, metric='cosine', shards=1) # create index\n",
    "\n",
    "id_vector_pairs = list(zip(ids, vectors)) # create id-vector pairs (ÇÜNKÜ UPSERT METHODU TEK INDEX İÇİN BİRDEN FAZLA VECTOR KABUL ETMİYOR)\n",
    "\n",
    "index = pinecone.Index(index_name=index_name) # get index\n",
    "\n",
    "index.upsert(vectors=id_vector_pairs) # insert vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRYING TO ASK QUESTION ABOUT EMBEDDED DATA\n",
    "\n",
    "import os\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "def answer_question(q):\n",
    "    llm = ChatOpenAI(\n",
    "        openai_api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "        model_name='gpt-3.5-turbo',\n",
    "        temperature=0.0\n",
    "    )\n",
    "    embeddings = model.wv.vectors\n",
    "    vector_store: Pinecone = Pinecone.from_existing_index(\n",
    "        index_name,\n",
    "        embeddings\n",
    "    )\n",
    "\n",
    "    qa_chain = load_qa_with_sources_chain(llm=llm, chain_type='stuff')\n",
    "    qa = RetrievalQAWithSourcesChain(combine_documents_chain=qa_chain, retriever=vector_store.as_retriever(search_type='similarity', search_kwargs={'k' : 3}), reduce_k_below_max_tokens=True)\n",
    "\n",
    "    answer_response = qa({\"question\": q}, return_only_outputs=True)\n",
    "\n",
    "    return answer_response\n",
    "\n",
    "q = \"İstanbul'da göz muayenesi yaptırmak istiyorum\"\n",
    "answer = answer_question(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMBEDDING THE JSON INTO PINECONE TRY 2 DONT LOOK HERE !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import pinecone\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('env/.env', override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "file_path='./data/structured/hospitals2.json'\n",
    "data = json.loads(Path(file_path).read_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Description': 'Akdeniz Şifa Hospital has started its activities to provide '\n",
      "                'health services to patients in Antalya and the surrounding '\n",
      "                'provinces. Akdeniz Şifa Hospital, which has agreements with '\n",
      "                'many private health insurance companies, also offers '\n",
      "                'contracted services within the scope of SGK in all branches.',\n",
      " 'Email': 'info@akdenizsifa.com',\n",
      " 'Location': 'Kuzey Yaka Mahallesi Varsak Köprüsü Yeşilırmak Caddesi No:367, '\n",
      "             '07060 Kepez/Antalya',\n",
      " 'Name': 'Akdeniz Şifa Hospital',\n",
      " 'Operations': 'IVF',\n",
      " 'Phone': '444 76 06',\n",
      " 'Ratings': 4.93,\n",
      " 'Specialty_Names': 'Orthopedics'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data = []\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "for item in data:\n",
    "    # Extract relevant text fields for embedding\n",
    "    text_to_embed = f\"{item['Name']} {item['Location']} {item['Description']} {item['Email']} {item['Phone']} {item['Operations']} {item['Specialty_Names']}\"\n",
    "\n",
    "    # Embed the text using OpenAIEmbeddings\n",
    "    embedding = embeddings.embed_query(text_to_embed)\n",
    "    \n",
    "    # Add the embedding to the item\n",
    "    item['embedding'] = embedding\n",
    "\n",
    "    # Append the item to the result\n",
    "    embedded_data.append(item)\n",
    "\n",
    "# Save or use the embedded data as needed\n",
    "pprint(embedded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "delete_all_pinecone_indexes()\n",
    "# Specify the Pinecone index name\n",
    "index_name = \"newindex\"  # Replace this with your desired index name\n",
    "\n",
    "# Initialize the Pinecone index\n",
    "index = pinecone.Index(index_name=index_name)\n",
    "\n",
    "# Upsert the embeddings into the Pinecone index\n",
    "for item in data:\n",
    "    document_id = item['Name']  # Use a unique identifier for each document\n",
    "    embedding = item['embedding']\n",
    "    \n",
    "    # Upsert the document into the Pinecone index\n",
    "    index.upsert([(document_id, embedding)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Akdeniz Şifa Hospital', 'Antalya Lara Anadolu Hospital']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "names_list = []\n",
    "for json_obj in data:\n",
    "  name = json_obj['Name']\n",
    "  names_list.append(name)\n",
    "names_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Description': 'Akdeniz Şifa Hospital has started its activities to provide '\n",
      "                'health services to patients in Antalya and the surrounding '\n",
      "                'provinces. Akdeniz Şifa Hospital, which has agreements with '\n",
      "                'many private health insurance companies, also offers '\n",
      "                'contracted services within the scope of SGK in all branches.',\n",
      " 'Email': 'info@akdenizsifa.com',\n",
      " 'Location': 'Kuzey Yaka Mahallesi Varsak Köprüsü Yeşilırmak Caddesi No:367, '\n",
      "             '07060 Kepez/Antalya',\n",
      " 'Name': 'Akdeniz Şifa Hospital',\n",
      " 'Operations': 'IVF',\n",
      " 'Phone': '444 76 06',\n",
      " 'Ratings': 4.93,\n",
      " 'Specialty_Names': 'Orthopedics'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/structured/hospitals2.json', \"r\") as file:\n",
    "  json_data = file.read()\n",
    "\n",
    "data_dict = json.loads(json_data)\n",
    "\n",
    "pprint(data_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Description': 'Akdeniz Şifa Hospital has started its activities to provide '\n",
      "                'health services to patients in Antalya and the surrounding '\n",
      "                'provinces. Akdeniz Şifa Hospital, which has agreements with '\n",
      "                'many private health insurance companies, also offers '\n",
      "                'contracted services within the scope of SGK in all branches.',\n",
      " 'Email': 'info@akdenizsifa.com',\n",
      " 'Location': 'Kuzey Yaka Mahallesi Varsak Köprüsü Yeşilırmak Caddesi No:367, '\n",
      "             '07060 Kepez/Antalya',\n",
      " 'Operations': 'IVF',\n",
      " 'Phone': '444 76 06',\n",
      " 'Ratings': 4.93,\n",
      " 'Specialty_Names': 'Orthopedics'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings = embeddings.embed_documents(names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "combined_list = [(names_list[i], n_embeddings[i], data_dict[i]) for i in range(len(names_list))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "main_list=[]\n",
    "first_100 = combined_list[:100]\n",
    "main_list.append(first_100)\n",
    "second_100 = combined_list[100:200]\n",
    "main_list.append(second_100)\n",
    "third_100 = combined_list[200:300]\n",
    "main_list.append(third_100)\n",
    "fourth_100 = combined_list[300:400]\n",
    "main_list.append(fourth_100)\n",
    "fifth_100 = combined_list[400:500]\n",
    "main_list.append(fifth_100)\n",
    "sixth_100 = combined_list[500:600]\n",
    "main_list.append(sixth_100)\n",
    "seventh_100 = combined_list[600:700]\n",
    "main_list.append(seventh_100)\n",
    "eighth_100 = combined_list[700:800]\n",
    "main_list.append(eighth_100)\n",
    "ninth_100 = combined_list[800:900]\n",
    "main_list.append(ninth_100)\n",
    "tenth_100 = combined_list[900:1000]\n",
    "main_list.append(tenth_100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(first_100))\n",
    "print(len(second_100))\n",
    "len(main_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "delete_all_pinecone_indexes()\n",
    "pinecone.init(api_key=os.environ.get('PINECONE_API_KEY'), environment=os.environ.get('PINECONE_ENV'))\n",
    "index_name = \"metadata-insert\" # put in the name of your pinecone index here\n",
    "\n",
    "index = pinecone.Index(\"metadata-insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.upsert(embedded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRYING BEFORE FRONT-END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"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\",\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Create a retriever from your Pinecone index\n",
    "retriever = index.as_retriever()\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# Now you can use this chain to ask questions to your database\n",
    "response = qa.run(\"I want a Glaucoma treatment in İstanbul what is my options\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
